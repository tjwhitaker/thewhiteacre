{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "%matplotlib inline\n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "plt.style.use('seaborn-whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent for a Linear Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_model(X, W):\n",
    "    # W is column vector\n",
    "    return X @ W[1:, :] + W[0, :]\n",
    "\n",
    "def rmse(model, X, T, W):\n",
    "    Y = model(X, W)\n",
    "    return np.sqrt(np.mean((T - Y)**2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let the output of our model be $Y$ and the error being minimized $E$.\n",
    "\n",
    "To perform gradient descent, we need $\\frac{\\partial E}{\\partial W}$. The calculation of this can be expressed with the chain rule.\n",
    "\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial W} = \\frac{\\partial E}{\\partial Y} \\frac{\\partial Y}{\\partial W}\n",
    "$$\n",
    "\n",
    "The error we want to minimize is the squared error, $(T - Y)^2$ and $Y = XW$.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial E}{\\partial W} &= \\frac{\\partial E}{\\partial Y} \\frac{\\partial Y}{\\partial W} \\\\\n",
    "\\frac{\\partial E}{\\partial W} &= \\frac{\\partial (T - Y)^2}{\\partial Y} \\frac{\\partial X W}{\\partial W} \\\\\n",
    "\\frac{\\partial E}{\\partial W} &= -2 (T - Y) X \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a simple model.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "   g(x;w) &= w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_D x_D \\\\\n",
    "   &= w_0 + \\sum_{i=1}^D w_i x_i \\\\\n",
    "   & = \\sum_{i=0}^D w_i x_i \\text{, where } x_0 = 1 \\\\\n",
    "   &= w^T x\\\\\n",
    "   &= x^T w\n",
    "\\end{align*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Force exerted by a spring is proportional to its length. The potential energy stored in the spring is proportional to the square of its length. So the position that minimizes the potential energy in the springs:\n",
    "\n",
    "\\begin{align*}\n",
    "\\sum_{n=1}^N (t_n - g(x_n;w))^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for a model g:\n",
    "\n",
    "$$\n",
    "g(x; w) = w_0 + w_1 x\n",
    "$$\n",
    "\n",
    "with parameters $$w = (w_0, w_1)$$ what gives the best fit?\n",
    "\n",
    "$$\n",
    "\\hat{w} = \\text{argmin}{w} \\sum_{n=1}^N (t_n - g(x_n ; w))^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect all targets into matrix $T$ and $x$ samples into matrix $X$ where $N$ is the number of samples and $D is the sample dimensionality.\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    T &= \\begin{bmatrix}\n",
    "      t_1 \\\\ t_2 \\\\ \\vdots \\\\ t_N\n",
    "    \\end{bmatrix} \\\\\n",
    "    X &= \\begin{bmatrix}\n",
    "      x_{1,0} & x_{1,1} & x_{1,2} & \\dotsc & x_{1,D} \\\\\n",
    "      x_{2,0} & x_{2,1} & x_{2,2} & \\dotsc & x_{2,D} \\\\\n",
    "      \\vdots \\\\\n",
    "      x_{N,0} & x_{N,1} & x_{N,2} & \\dotsc & x_{N,D}\n",
    "    \\end{bmatrix}\\\\\n",
    "    w &= \\begin{bmatrix} w_0 \\\\ w_1 \\\\ \\vdots \\\\ w_D \\end{bmatrix}\n",
    "  \\end{align*}\n",
    "  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collection of all differences is $T−Xw$, which is an $N×1$ matrix. To form the square of all values and add them up, just do a dot product $(T−Xw)^T(T−Xw)$. \n",
    "\n",
    "This only works if the value we are predicting is a scalar, which means $T$ is a column matrix. If we want to predict more than one value for each sample, $T$ will have more than one column. Let's continue with the derivation assuming $T$ has $K$ columns, meaning we want a linear model with $K$ outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find the best value for $w$, we take the derivative of the sum of squared error objective, set it equal to 0 and solve for $w$. \n",
    "\n",
    "Here $xn$ is one sample as a column vector, so it must be transposed to a row vector before being multiplied by the column vector $w$.\n",
    "\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (t_n - g(x_n;w))^2}{\\partial w} &= -2 \\sum_{n=1}^N (t_n - g(x_n;w) \\frac{\\partial g(x_n;w)}{\\partial w}\\\\\n",
    "&= -2 \\sum_{n=1}^N (t_n - x_n^T w) \\frac{\\partial x_n^T w}{\\partial w}\\\\\n",
    "&= -2 \\sum_{n=1}^N (t_n - x_n^T w) x_n^T\n",
    "\\end{align*}\n",
    "\n",
    "We can then express the sum with a dot product:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial \\sum_{n=1}^N (t_n - g(x_n;w))^2}{\\partial w} \n",
    "&= -2 \\sum_{n=1}^N (t_n - x_n^T w) x_n^T\\\\\n",
    "&= -2 X^T (T - X w)\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we set this all to 0 and solve for w.\n",
    "\n",
    "\\begin{align*}\n",
    "-2 X^T (T - X w) &= 0\\\\\n",
    "X^T (T - X w) &= 0\\\\\n",
    "X^T T &= X^T X w\\\\\n",
    "w &= (X^T X)^{-1} X^T T\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental algorithm\n",
    "\n",
    "But what if we have millions of samples? $X$ and $T$ can get very large.\n",
    "\n",
    "We can derive a sequential algorithm for finding $w$, as the derivative of a sum is the sum of the derivatives. We can express this as a gradient which is a vector or matrix of derivatives.\n",
    "\n",
    "\\begin{align*}\n",
    "g(x_n, w) &= w_0 + w_1 x_{n,1} + w_2 x_{n,2} + \\cdots + w_D x_{n,D} = x_n^T w\\\\\n",
    "E(X, T, w) &= \\sum_{n=1}^N (t_n - g(x_n, w))^2\\\\\n",
    "\\nabla_w E(X, T, w) &= \\nabla_w \\left ( \\sum_{n=1}^N (t_n - g(x_n, w))^2 \\right )\\\\\n",
    "&= \n",
    "\\sum_{n=1}^N \\nabla_w (t_n - g(x_n, w))^2\\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(x_n, w)) \\nabla_w (t_n - g(x_n, w)) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(x_n, w)) (-1) \\nabla_w g(x_n, w) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(x_n, w)) (-1) \\nabla_w (x_n^T w) \\\\\n",
    "&= \n",
    "\\sum_{n=1}^N 2 (t_n - g(x_n, w)) (-1) x_n \\\\\n",
    "&= \n",
    "-2 \\sum_{n=1}^N (t_n - g(x_n, w))  x_n \\\\\n",
    "\\end{align*}\n",
    "\n",
    "Now the weights for the next time step are updated based on the gradient of $E$ for that sample. This is called stochastic approximation where $\\rho$ is a scalar step size.\n",
    "\n",
    "\\begin{align*}\n",
    "w^{(k+1)} &= w^{(k)}  + \\rho \\; x_n \\; (t_n^T - g(x_n, w))) \n",
    "\\end{align*}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

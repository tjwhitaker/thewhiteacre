{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "metadata"
    ]
   },
   "source": [
    "#+TITLE: Quantization\n",
    "#+CATEGORIES: compression\n",
    "#+DESCRIPTION: Quantization is used to reduce the precision of the weights and biases in a model in order to decrease computational requirements. It involves converting full-precision 32-bit weights into lower-precision formats. Typically 16-bit or 8-bit quantization is used, but research has shown promise in resource constrained enviroments for ternary and binary networks.\n",
    "#+DATE: 2020-01-02\n",
    "#+HERO: /static/space-bg.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "Quantization is an effective technique for significantly reducing the computational requirements of large models by training models with lower precision parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "# Standard Full Precision Model\n",
    "class LeNet(nn.Module):\n",
    "    def __init__(self, in_channels=1, n_outputs=10):\n",
    "        super(LeNet, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(in_channels, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=3)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, n_outputs)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "model = LeNet().to(device)\n",
    "print(model.conv1.weight.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch offers an easy method to convert to half precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.25 MB\n",
      "0.13 MB\n"
     ]
    }
   ],
   "source": [
    "def print_model_size(mdl):\n",
    "    torch.save(mdl.state_dict(), \"tmp.pt\")\n",
    "    print(\"%.2f MB\" %(os.path.getsize(\"tmp.pt\")/1e6))\n",
    "    os.remove('tmp.pt')\n",
    "\n",
    "print_model_size(model)\n",
    "model.half()\n",
    "print_model_size(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also have to make sure the input data is half precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = torchvision.transforms.Compose(\n",
    "    [torchvision.transforms.ToTensor(),\n",
    "     torchvision.transforms.ConvertImageDtype(torch.float16),\n",
    "     torchvision.transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST('../data', train=True, transform=transforms, download=False)\n",
    "test_dataset = torchvision.datasets.MNIST('../data', train=False, transform=transforms, download=False)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model,  optimizer, criterion, train_loader, epochs=1):\n",
    "    model.train()\n",
    "    for i in range(epochs):\n",
    "        train_loss = 0\n",
    "        for _, (inputs, target) in enumerate(train_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            prediction = model(inputs)\n",
    "\n",
    "            loss = criterion(prediction, target)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            model.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {i}, NLL: {train_loss / len(train_loader.dataset)}\")\n",
    "\n",
    "def test(model, optimizer, criterion, test_loader):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        num_correct = 0\n",
    "        test_loss = 0\n",
    "        for _, (inputs, target) in enumerate(test_loader):\n",
    "            inputs = inputs.to(device)\n",
    "            target = target.to(device)\n",
    "\n",
    "            output = model(inputs)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            _, predictions = torch.max(output, -1)\n",
    "\n",
    "            num_correct += (predictions == target).sum().data.item()\n",
    "\n",
    "        accuracy = (num_correct / len(test_loader.dataset)) * 100\n",
    "    print(f\"Test Accuracy: {accuracy}, NLL: {test_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, NLL: 0.041533625284830726\n",
      "Test Accuracy: 90.52, NLL: 96.3634033203125\n"
     ]
    }
   ],
   "source": [
    "train(model, optimizer, criterion, train_loader, epochs=1)\n",
    "test(model, optimizer, criterion, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, most articles on this topic revolve around post-training quantization. Pyorch implements the following techniques.\n",
    "\n",
    "1. dynamic quantization (weights quantized with activations read/stored in floating point and quantized for compute)\n",
    "\n",
    "2. static quantization (weights quantized, activations quantized, calibration required post training)\n",
    "\n",
    "3. static quantization aware training (weights quantized, activations quantized, quantization numerics modeled during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "df0893f56f349688326838aaeea0de204df53a132722cbd565e54b24a8fec5f6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

<!DOCTYPE html><html><head><title>Models | Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/models.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Models</h1><p>A catalog of important neural network architectures through history.</p></div></div><div class="catalog"><div class="wrapper"><div class="item"><h1>Perceptron (1958)</h1><p><small>Frank Rosenblatt</small></p><p>Neural network research begins with the first implementation of an artificial neuron called the perceptron. The theory for the perceptron was introduced in 1943 by McCulloch and Pitts as a binary threshold classifier. The first implementation was actually intended to be a machine rather than a program. Photocells were interconnected with potentiometers that were updated during learning with electric motors.</p><div class="links"><a href="/static/perceptron.pdf">Paper</a></div></div><div class="item"><h1>Neocognitron (1979)</h1><p><small>Kunihiko Fukushima</small></p><p>A lesser known piece of history. The earliest inspiration for convolutional neural networks comes from Japan in the midst of the AI winter. A breakthrough development for computer vision, the neocognitron utilizes alternating layers of locally connected feature extraction and positional shift cells.</p><div class="links"><a href="/static/neocognitron.pdf">Paper</a></div></div><div class="item"><h1>Hopfield Network (1982)</h1><p><small>John Hopfield</small></p><p>The Hopfield network is a type of recurrent network based on bidirectional connections between neurons. Hopfield networks are a type of Ising model (also known as spin glass) that use Hebbian learning to train associative memory systems.</p><div class="links"><a href="/static/hopfield.pdf">Paper</a></div></div><div class="item"><h1>Boltzmann Machine (1985)</h1><p><small>David Ackley, Geoffrey Hinton, Terrence Sejnowski</small></p><p>The Boltzmann Machine is a type of probabilistic graph model that consists of binary stochastic units and weights learned through an energy based minimization algorithm called contrastive divergence. The name comes from using the Boltzmann distribution to model the joint probability distribution of the input data.</p><div class="links"><a href="/static/boltzmann.pdf">Paper</a></div></div><div class="item"><h1>LeNet (1989)</h1><p><small>Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, Lawrence Jackel</small></p><p>LeNet is the earliest introduction of the modern convolutional architecture. It contains three alternating convolutional and pooling layers followed by two fully connected layers. This model was introduced alongside the back propagation algorithm which produced incredible results at the time for image classification.</p><div class="links"><a href="/static/lenet.pdf">Paper</a></div></div><div class="item"><h1>Long Short-Term Memory (1991)</h1><p><small>Sepp Hochreiter, JÃ¼rgen Schmidhuber</small></p><p>The LSTM is a recurrent network that contains a memory cell and three gating units (input, output, and forget) that regulate the flow of information into and out of the cell. The memory cell can selectively remember or forget information over long periods of time.</p><div class="links"><a href="/static/lstm.pdf">Paper</a></div></div><div class="item"><h1>Echo State Network (2002)</h1><p><small>Herbert Jaeger</small></p><p>The echo state network is a type of recurrent architecture that utilizes a reservoir of fixed and randomly weighted connections that remain frozen during training. Only layers outside of the reservoir are optimized. The dynamic behavior of the reservoir generates complex temporal patterns which create an echo of the input signal.</p><div class="links"><a href="/static/echo-state.pdf">Paper</a></div></div><div class="item"><h1>Liquid State Machine (2004)</h1><p><small>Wolfgang Maass, Henry Markram</small></p><p>The liquid state machine is closely related to the echo state network but utilizes a reservoir of randomly connected spiking neurons. The dynamic reservoir produces complex spatio-temporal activation patterns that are read out by linear discriminant units.</p><div class="links"><a href="/static/liquid-state.pdf">Paper</a></div></div><div class="item"><h1>AlexNet (2012)</h1><p><small>Alex Krishevsky, Ilya Sutskever, Geoffrey Hinton</small></p><p>The model that kickstarted the deep learning revolution. Consisting of eight layers, including five convolutional layers and three fully connected layers. AlexNet was one of the first deep convolutional neural networks to achieve state-of-the-art results on ImageNet. The original implementation split the network over two independent gpus to alleviate challenges with memory at the time.</p><div class="links"><a href="/static/alexnet.pdf">Paper</a></div></div><div class="item"><h1>VGG (2014)</h1><p><small>Karen Simonyan, Andrew Zisserman</small></p><p>The Visual Geometry Group developed this network architecture that is characterized by its simplicity and uniformity. This model popularized the 3x3 convolutional filter size and its design has influenced the development of modern convolutional architectures.</p><div class="links"><a href="/static/vgg.pdf">Paper</a></div></div><div class="item"><h1>Inception (2014)</h1><p><small>Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, Andrew Rabinovich</small></p><p>Inception is a deep convolutional network developed by Google researchers. It introduced the concept of inception modules, which are composed of multiple parallel convolutional layers of different kernel sizes to capture different feature scales.</p><div class="links"><a href="/static/inception.pdf">Paper</a></div></div><div class="item"><h1>ResNet (2016)</h1><p><small>Kaiming He, Xiangyu Zhang, Shaoqing Ren, Jian Sun</small></p><p>Residual Networks popularized skip connections in deep networks to alleviate vanishing gradient problems. The shortcut connections between blocks allows the gradients to flow directly from later layers to earlier layers, enabling depths of over a hundred layers.</p><div class="links"><a href="/static/resnet.pdf">Paper</a></div></div><div class="item"><h1>DenseNet (2016)</h1><p><small>Gao Huang, Zhuang Liu, Laurens van der Maaten, Kilian Weinberger</small></p><p>DenseNet builds on top of the ideas introduced in residual networks by implementing skip connections between a given layer and every other successive layer following it. Each layer receives a concatenation of all the feature maps of all the layers preceding it. This allows for learning at different levels of abstraction throughout the network.</p><div class="links"><a href="/static/densenet.pdf">Paper</a></div></div><div class="item"><h1>SqueezeNet (2016)</h1><p><small>Forrest Iandola, Song Han, Matthew Moskewicz, Khalid Ashraf, William Dally, Kurt Keutzer</small></p><p>SqueezeNet is notable for achieving comparable accuracy to larger neural network architectures while using significantly fewer parameters. SqueezeNet utilizes fire modules that consist of a 1x1 convolutional squeeze layer followed by a 3x3 convolutional expand layer.</p><div class="links"><a href="/static/squeezenet.pdf">Paper</a></div></div><div class="item"><h1>ShuffleNet (2017)</h1><p><small>Xiangyu Zhang, Xinyu Zhou, Mengxiao Lin, Jian Sun</small></p><p>ShuffleNet is a mobile optimized network that uses a combination of depthwise separable convolutions, channel shuffle operations, and pointwise convolutions to greatly reduce computational cost while maintaining accuracy.</p><div class="links"><a href="/static/shufflenet.pdf">Paper</a></div></div><div class="item"><h1>Transformer (2017)</h1><p><small>Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, Illia Polosukhin</small></p><p>These models replace the traditional recurrent and convolutional networks with a self attention mechanism that allows the model to attend to different parts of the input sequence to generate contextual representations. The transformer consists of an encoder and decoder architecture that perform input/output mapping.</p><div class="links"><a href="/static/transformer.pdf">Paper</a></div></div><div class="item"><h1>EfficientNet (2019)</h1><p><small>Mingxing Tan, Quoc Le</small></p><p>EfficientNet uses a compound scaling method that optimizes the depth, width, and resolution of the network for a given amount of computational resources. EfficienNet utilizes a combination of residual inverted bottlenecks and squeeze-and-excitation blocks.</p><div class="links"><a href="/static/efficientnet.pdf">Paper</a></div></div><div class="item"><h1>MLP Mixer (2021)</h1><p><small>Ilya Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, Alexey Dosovitskiy</small></p><p>A new take on vision architectures that eschew convolution and attention for only fully connected layers. MLP mixer applies spatial embeddings to the input image and then applies permutation-invariant operations through channel and token mixing layers.</p><div class="links"><a href="/static/mlp-mixer.pdf">Paper</a></div></div></div></div></main><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>. This website's <a href="https://github.com/tjwhitaker/thewhiteacre">source code</a> was written entirely in scheme!</p></div><script src="/static/prism.js"></script><script src="/static/prism-python.min.js"></script><script src="/static/prism-julia.min.js"></script><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer></div></body></html>
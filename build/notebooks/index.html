<!DOCTYPE html><html><head><title>Notebooks | Wits End</title><meta charset="utf-8" /><meta name="viewport" content="width=device-width, initial-scale=1" /><link rel="icon" href="/static/favicon.ico" /><link rel="stylesheet" href="/static/style.css" /><link rel="stylesheet" href="/static/notebooks.css" /><link rel="preconnect" href="https://fonts.googleapis.com" /><link rel="preconnect" href="https://fonts.gstatic.com" /><link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans:wght@500;700&amp;display=block" /><link rel="stylesheet" href="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.css" /><script src="https://api.mapbox.com/mapbox-gl-js/v2.1.1/mapbox-gl.js"></script></head><body><div class="home"><header><div class="wrapper"><a href="/" class="logo"><img src="/static/logo.svg" /></a><nav><a href="/cv">CV</a><a href="/research">Publications</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a></nav></div></header><main><div class="hero"><div class="overlay"></div><div class="wrapper"><h1>Notebooks</h1><p>There have been many algorithmic and mathematical techniques introduced in order to better optimize neural networks. This is a compliation of articles and tutorials designed to dig into some of these finer details.</p></div></div><div class="feed"><div class="wrapper"><div class="filters"><nav class="categories"><a href="#" class="active">All</a><a href="#">Architecture</a><a href="#">Compression</a><a href="#">Evolution</a><a href="#">Optimization</a><a href="#">Regularization</a></nav></div><div class="post"><small>2020-01-01</small><h1>Activation Functions</h1><p>Activation functions play a crucial role in determining the output of neurons in neural networks. They introduce non-linearity into the network, enabling it to model complex relationships between inputs and outputs.</p><a href="/notebooks/activation-functions">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Quantization</h1><p>Quantization is used to reduce the precision of the weights and biases in a model in order to decrease computational requirements. It involves converting full-precision 32-bit weights into lower-precision formats. Typically 16-bit or 8-bit quantization is used, but research has shown promise in resource constrained enviroments for ternary and binary networks.</p><a href="/notebooks/quantization">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Data Augmentation</h1><p>A strategy used in machine learning to increase the diversity and amount of training data without actually collecting new data. It involves creating modified versions of existing data using techniques like rotation, scaling, flipping, cropping, and brightness or color adjustments.</p><a href="/notebooks/data-augmentation">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Learning Rate Schedules</h1><p>These strategies adjust the learning rate, and/or other optimizer parameters, throughout the training process. Generally, large learning rates are used in the early stages of training where large steps can be taken to improve efficiency before slowly decaying to small rates to encourage fine-grained convergence.</p><a href="/notebooks/learning-rate-schedules">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Convolutions</h1><p>Convolutions are mathematical operations that process input data using filters or kernels. The operation involves sliding the filter over the input data and performing an element-wise multiplication between the filter weights and the input window. These filters often learn to detect edges, shapes, or textures in image data in a translation invariant way.</p><a href="/notebooks/convolutions">Read More</a></div><div class="post"><small>2020-01-02</small><h1>Weight Pruning</h1><p>Pruning is used to reduce the complexity and size of a model by removing weights or neurons. Pruning methods typically select weights to prune according to importance heuristics like magnitude or gradient saliency. However, even random pruning has been shown to produce accurate models at significant levels of sparsity. While pruning can cause some loss in model accuracy, this can be mitigated by fine-tuning the pruned model on the original dataset.</p><a href="/notebooks/weight-pruning">Read More</a></div></div></div></main><div class="reference"><p>Hero images generated with neural networks via <a href="https://midjourney.com">midjourney</a>.</p></div><footer><div class="wrapper"><div class="column"><h2>Site</h2><a href="/about">About</a><a href="/models">Models</a><a href="/notebooks">Notebooks</a><a href="/research">Research</a></div><div class="column"><h2>Links</h2><a href="https://arxiv-sanity-lite.com">Arxiv Sanity</a><a href="https://paperswithcode.com">Papers With Code</a><a href="https://news.ycombinator.com">Hacker News</a><a href="https://www.youtube.com/c/pbsspacetime">Space Time</a></div><div class="column"><h2>Self</h2><a href="#">CV</a><a href="https://github.com/tjwhitaker">Github</a><a href="https://lichess.org/@/tjwhitaker">Lichess</a><a href="https://orcid.org/0000-0003-3792-3901">Orcid</a></div></div></footer></div></body></html>